{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Action Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "\n",
    "from animalai.envs.environment import AnimalAIEnvironment\n",
    "from mlagents_envs.envs.unity_gym_env import UnityToGymWrapper\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from aaisrc.aaiagents import RandomActionAgent\n",
    "from aaisrc.aaiyaml import find_yaml_files\n",
    "from aaisrc.aaiyaml import yaml_combinor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_randomactionagent(env_path : str, configuration_file : str, save_path : str, watch : bool = False, num_runs : int = 100, verbose : bool = True):\n",
    "    episode_rewards = []\n",
    "\n",
    "    versionThreeRandomaActionAgent = RandomActionAgent(max_step_length=10,\n",
    "                                             step_length_distribution='normal',\n",
    "                                             norm_mu=5,\n",
    "                                             norm_sig=1)\n",
    "    \n",
    "    port = 4000 + random.randint(\n",
    "    0, 1000\n",
    "    )  # use a random port to avoid problems if a previous version exits slowly\n",
    "    \n",
    "    aai_env = AnimalAIEnvironment( \n",
    "        inference=watch, #Set true when watching the agent\n",
    "        seed = 2023,\n",
    "        worker_id=random.randint(0, 65500),\n",
    "        file_name=env_path,\n",
    "        arenas_configurations=configuration_file,\n",
    "        base_port=port,\n",
    "        useCamera=False,\n",
    "        useRayCasts=False,\n",
    "        no_graphics=True,\n",
    "        timescale=1\n",
    "    )\n",
    "\n",
    "    env = UnityToGymWrapper(aai_env, uint8_visual=False, allow_multiple_obs=True, flatten_branched=True)\n",
    "\n",
    "    \n",
    "\n",
    "    random.seed(2023)\n",
    "\n",
    "    for _episode in range(num_runs): \n",
    "        if verbose:\n",
    "            print(f\"Running episode {_episode+1} of {num_runs}.\") \n",
    "        \n",
    "        obs = env.reset()\n",
    "        \n",
    "        done = False\n",
    "        episodeReward = 0\n",
    "        initialActionAgent = versionThreeRandomaActionAgent\n",
    "        initialActionAgent.prev_step_bias = 0 #select a random action according to the biases. There is no previous step bias as there is no previous step at the start of an episode!\n",
    "\n",
    "        previous_action = initialActionAgent.get_new_action(prev_step=0) \n",
    "\n",
    "        while not done:\n",
    "\n",
    "            step_list = versionThreeRandomaActionAgent.get_num_steps(prev_step = previous_action)\n",
    "            \n",
    "            for action in step_list:\n",
    "                \n",
    "                obs, reward, done, info = env.step(int(action))\n",
    "                episodeReward += reward\n",
    "                env.render()\n",
    "\n",
    "                previous_action = action\n",
    "\n",
    "                if done:\n",
    "                    print(F\"Episode Reward: {episodeReward}\")\n",
    "                    obs=env.reset()\n",
    "                    #env.close()\n",
    "                    episode_rewards.append(episodeReward)\n",
    "                    break\n",
    "                    \n",
    "            if not done:\n",
    "                ## get new action for one step before repeating while loop.\n",
    "\n",
    "                action = versionThreeRandomaActionAgent.get_new_action(prev_step = previous_action)\n",
    "                \n",
    "                obs, reward, done, info = env.step(int(action))\n",
    "                \n",
    "                episodeReward += reward\n",
    "                env.render()\n",
    "\n",
    "                previous_action = action\n",
    "\n",
    "                if done:\n",
    "                    print(F\"Episode Reward: {episodeReward}\")\n",
    "                    obs=env.reset()\n",
    "                    #env.close()\n",
    "                    episode_rewards.append(episodeReward)\n",
    "                    break #to be sure.\n",
    "        \n",
    "    env.close()\n",
    "    \n",
    "    results_dataframe = pd.DataFrame({\"EpisodeNumber\" : [x for x in range(num_runs)],\n",
    "                                     \"FinalReward\" : episode_rewards})\n",
    "    \n",
    "    results_dataframe.to_csv(save_path,\n",
    "                            index = False)\n",
    "\n",
    "    return results_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_randomactionagent_episode(env_path : str, configuration_folder : str, tmp_file_location : str, save_path_csv : str, watch : bool = False, seed : int = 2023, batch_size: int = 50, port_base : int = 6600):\n",
    "    episode_rewards = []\n",
    "\n",
    "    versionThreeRandomaActionAgent = RandomActionAgent(max_step_length=10,\n",
    "                                             step_length_distribution='normal',\n",
    "                                             norm_mu=5,\n",
    "                                             norm_sig=1)\n",
    "    \n",
    "    yaml_index = 0\n",
    "\n",
    "    port = port_base + yaml_index\n",
    "        \n",
    "    batch_counter = 0\n",
    "\n",
    "    paths, names = find_yaml_files(configuration_folder)\n",
    "\n",
    "    \n",
    "    for yaml_index in range(0, len(paths), batch_size):\n",
    "\n",
    "        completed = False\n",
    "\n",
    "        while not completed:\n",
    "            try:\n",
    "                if (yaml_index + batch_size) > len(paths) or batch_size > len(paths):\n",
    "                    upper_bound = len(paths)\n",
    "                else:\n",
    "                    upper_bound = ((yaml_index + batch_size))\n",
    "                \n",
    "                print(f\"Running inferences on batch {batch_counter + 1} of {batch_size} files of total {len(paths)}. {len(paths) - (batch_size * (batch_counter + 1))} instances to go.\")\n",
    "\n",
    "                batch_files = paths[yaml_index:upper_bound]\n",
    "\n",
    "                batch_file_names = names[yaml_index:upper_bound]\n",
    "\n",
    "                batch_temp_file_name = f\"TempConfig_RandomAction_{seed}_{yaml_index}.yml\"\n",
    "\n",
    "                config_file_path = yaml_combinor(file_list = batch_files, temp_file_location=tmp_file_location, stored_file_name = batch_temp_file_name)\n",
    "            \n",
    "                aai_env = AnimalAIEnvironment( \n",
    "                    inference=watch, #Set true when watching the agent\n",
    "                    seed = seed,\n",
    "                    worker_id=random.randint(0, 65500),\n",
    "                    file_name=env_path,\n",
    "                    arenas_configurations=config_file_path,\n",
    "                    base_port=port,\n",
    "                    useCamera=False,\n",
    "                    useRayCasts=False,\n",
    "                    no_graphics=True,\n",
    "                    timescale=1\n",
    "                )\n",
    "\n",
    "                env = UnityToGymWrapper(aai_env, uint8_visual=False, allow_multiple_obs=True, flatten_branched=True)\n",
    "\n",
    "            \n",
    "\n",
    "                random.seed(seed)\n",
    "\n",
    "                for _episode in range(batch_size): \n",
    "                    print(f\"Running episode {_episode+1} of {batch_size}.\") \n",
    "\n",
    "                    obs = env.reset()\n",
    "                \n",
    "                    done = False\n",
    "                    episodeReward = 0\n",
    "                    initialActionAgent = versionThreeRandomaActionAgent\n",
    "                    initialActionAgent.prev_step_bias = 0 #select a random action according to the biases. There is no previous step bias as there is no previous step at the start of an episode!\n",
    "\n",
    "                    previous_action = initialActionAgent.get_new_action(prev_step=0) \n",
    "\n",
    "                    while not done:\n",
    "\n",
    "                        step_list = versionThreeRandomaActionAgent.get_num_steps(prev_step = previous_action)\n",
    "                    \n",
    "                        for action in step_list:\n",
    "                        \n",
    "                            obs, reward, done, info = env.step(int(action))\n",
    "                            episodeReward += reward\n",
    "                            env.render()\n",
    "\n",
    "                            previous_action = action\n",
    "\n",
    "                            if done:\n",
    "                                print(F\"Episode Reward: {episodeReward}\")\n",
    "                                obs=env.reset()\n",
    "                                #env.close()\n",
    "                                episode_rewards.append(episodeReward)\n",
    "                                break\n",
    "                            \n",
    "                        if not done:\n",
    "                            ## get new action for one step before repeating while loop.\n",
    "\n",
    "                            action = versionThreeRandomaActionAgent.get_new_action(prev_step = previous_action)\n",
    "                        \n",
    "                            obs, reward, done, info = env.step(int(action))\n",
    "                        \n",
    "                            episodeReward += reward\n",
    "                            env.render()\n",
    "\n",
    "                            previous_action = action\n",
    "\n",
    "                            if done:\n",
    "                                print(F\"Episode Reward: {episodeReward}\")\n",
    "                                obs=env.reset()\n",
    "                                #env.close()\n",
    "                                episode_rewards.append(episodeReward)\n",
    "                                break #to be sure.\n",
    "                        \n",
    "                    file_exists = os.path.isfile(save_path_csv)\n",
    "                    with open(save_path_csv, 'a' if file_exists else 'w', newline='') as csv_file:\n",
    "                        csv_write = csv.writer(csv_file)\n",
    "                        if not file_exists:\n",
    "                            csv_write.writerow(['episode', 'finalReward'])\n",
    "\n",
    "                        csv_write.writerow([str(names[yaml_index+_episode]), str(episodeReward)])\n",
    "                        print(f\"Writing episode score {episodeReward} for episode {names[yaml_index+_episode]} to {save_path_csv}\")\n",
    "                env.close()\n",
    "                os.remove(config_file_path)\n",
    "                batch_counter += 1\n",
    "                completed = True\n",
    "            except:\n",
    "                print(\"Episode failed. Retrying\")\n",
    "                port += 1\n",
    "                try: \n",
    "                    env.close()\n",
    "                except:\n",
    "                    pass\n",
    "                completed = False\n",
    "                break\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAA_results_foraging = evaluate_randomactionagent(env_path = \"../env/AnimalAI.exe\",\n",
    "                                       configuration_file = \"../configs/foragingTask/foragingTaskSpawnerTree.yml\",\n",
    "                                       save_path = \"../analysis/data/foraging/randomActionAgent100Foraging.csv\",\n",
    "                                       num_runs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAA_results_button = evaluate_randomactionagent(env_path = \"../env/AnimalAI.exe\",\n",
    "                                       configuration_file = \"../configs/buttonPressTask/buttonPressGreen.yml\",\n",
    "                                       save_path = \"../analysis/data/buttonPress/randomActionAgent100Button.csv\",\n",
    "                                       num_runs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [9022, 8812, 1056, 9917, 1942]\n",
    "\n",
    "for seed in seeds:\n",
    "    save_path = f\"../analysis/data/competitionAAITestbed/RandomActionAgent_{seed}.csv\"\n",
    "    eval = evaluate_randomactionagent(env_path=\"../env/AnimalAI.exe\",\n",
    "                                      configuration_folder=\"../configs/competition\",\n",
    "                                      tmp_file_location=\"../..\",\n",
    "                                      save_path_csv=save_path,\n",
    "                                      seed = seed,\n",
    "                                      port_base = seed + random.randint(0, 1000),\n",
    "                                      batch_size=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "animalaiv3.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
