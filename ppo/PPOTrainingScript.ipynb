{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO Training Script\n",
    "\n",
    "A template script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "import torch as th\n",
    "\n",
    "import random\n",
    "\n",
    "from mlagents_envs.envs.unity_gym_env import UnityToGymWrapper\n",
    "from animalai.envs.environment import AnimalAIEnvironment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent_single_config(configuration_file, env_path , results_path, log_bool = False, aai_seed = 2023, watch = False, num_saves = 100, num_steps = 10000):\n",
    "    \n",
    "    port = 5005 + random.randint(\n",
    "    0, 1000\n",
    "    )  # use a random port to avoid problems if a previous version exits slowly\n",
    "    \n",
    "    if not log_bool:\n",
    "        log_folder_path = \"\"\n",
    "    else:\n",
    "        log_folder_path = results_path + \"/player_logs\"\n",
    "\n",
    "    aai_env = AnimalAIEnvironment(\n",
    "        seed = aai_seed,\n",
    "        file_name=env_path,\n",
    "        log_folder = log_folder_path,\n",
    "        arenas_configurations=configuration_file,\n",
    "        play=False,\n",
    "        base_port=port,\n",
    "        inference=watch,\n",
    "        useCamera=False,\n",
    "        #resolution=64,\n",
    "        useRayCasts=True,\n",
    "        no_graphics=True,\n",
    "        raysPerSide=15,\n",
    "        rayMaxDegrees = 30,\n",
    "        timescale=1,\n",
    "        \n",
    "    )\n",
    "\n",
    "\n",
    "    env = UnityToGymWrapper(aai_env, uint8_visual=False, allow_multiple_obs=False, flatten_branched=True)\n",
    "    runname = \"competition_raycast_ppo\"\n",
    "\n",
    "    policy_kwargs = dict(activation_fn=th.nn.ReLU)\n",
    "    model = PPO(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, verbose=1, tensorboard_log=(results_path + \"/tensor_log/\" + runname))\n",
    "\n",
    "    reset_num_timesteps = True\n",
    "    for i in range(num_saves):\n",
    "        model.learn(num_steps, reset_num_timesteps=reset_num_timesteps)\n",
    "        model.save(results_path + \"/modelsaves/\" + runname + \"/model_\" + str( (i+1)*num_steps ))\n",
    "        reset_num_timesteps = False\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_path = \"../env/AnimalAI.exe\"\n",
    "model_results_path = \"../modelsaves\"\n",
    "configuration_file = \"../configs/aai-competition-curriculum.yml\"\n",
    "\n",
    "train_agent_single_config(configuration_file=configuration_file, env_path = env_path, results_path = model_results_path, watch = False, num_saves = 10, num_steps = 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "run:\n",
    "tensorboard --logdir ./tensor_log\n",
    "\n",
    "from command line in conda environment to view tensor log (allows you to watch the agent while it trains).\n",
    "(change the path to wherever you are storing tensor logs)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "animalaiv3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a43ad5049bc12cdb7a231109f90c0e4b3912f1bf6225b3b59bbf4f1bf238a4ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
